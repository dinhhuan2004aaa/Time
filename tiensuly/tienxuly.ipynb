{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CaMVZgC5NQrn",
        "outputId": "18875dd3-5654-4af1-849f-9185f4a00d1e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7da8338"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def process_station_data(file_path):\n",
        "    \"\"\"\n",
        "    Reads, processes, and cleans station data from a single file.\n",
        "\n",
        "    Args:\n",
        "        file_path (str): The full path to the data file.\n",
        "\n",
        "    Returns:\n",
        "        pandas.DataFrame: The processed DataFrame, or None if the file is discarded.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # 1. Check for missing values (initial)\n",
        "        # print(f\"Initial missing values in {os.path.basename(file_path)}:\\n{df.isnull().sum()}\")\n",
        "\n",
        "        # 2. Process 'mask' column\n",
        "        mask_cols = ['tmp', 'dew', 'wnd_angle', 'wnd_rate', 'slp']\n",
        "        if 'mask' in df.columns:\n",
        "            for i, col in enumerate(mask_cols):\n",
        "                if col in df.columns:\n",
        "                    # Assuming 'mask' is a string of 5 characters ('0' or non-'0')\n",
        "                    # and corresponds to the order tmp, dew, wnd_angle, wnd_rate, slp\n",
        "                    # This part might need adjustment based on the actual format of your 'mask'\n",
        "                    df.loc[df['mask'].str[i] != '0', col] = np.nan\n",
        "        else:\n",
        "            print(f\"Warning: 'mask' column not found in {os.path.basename(file_path)}\")\n",
        "\n",
        "\n",
        "        # Convert 'time' to datetime and sort\n",
        "        if 'time' in df.columns:\n",
        "            df['time'] = pd.to_datetime(df['time'])\n",
        "            df = df.sort_values(by='time').reset_index(drop=True)\n",
        "\n",
        "            # 3. Process 'time_diff' and add dummy rows\n",
        "            if 'time_diff' in df.columns:\n",
        "                # Assuming 'time_diff' is in minutes or a unit that can be converted to Timedelta\n",
        "                # You might need to adjust the unit based on your data\n",
        "                df['time_diff'] = pd.to_timedelta(df['time_diff'], unit='m')\n",
        "\n",
        "                # Detect missing time intervals and add dummy rows\n",
        "                # This is a simplified approach. A more robust method might be needed\n",
        "                # depending on the expected frequency of your data.\n",
        "                # For example, if data should be every 10 minutes:\n",
        "                # expected_interval = pd.Timedelta(minutes=10)\n",
        "                # missing_intervals = df['time'].diff() > expected_interval\n",
        "                # print(f\"Missing intervals detected in {os.path.basename(file_path)}: {missing_intervals.sum()}\")\n",
        "\n",
        "                # A more general approach to reindex and fill missing timestamps:\n",
        "                if not df.empty:\n",
        "                    start_time = df['time'].min()\n",
        "                    end_time = df['time'].max()\n",
        "                    # Assuming data should be at regular intervals. Replace '10T' with your actual frequency (e.g., 'H' for hourly)\n",
        "                    # You might need to infer the frequency or get it from metadata.\n",
        "                    # For demonstration, let's assume a frequent interval (e.g., 1 minute) to catch small gaps.\n",
        "                    # A more realistic interval based on 'time_diff' distribution would be better.\n",
        "                    # Let's try to infer the mode of the time differences as the expected interval\n",
        "                    time_diffs = df['time'].diff().dropna()\n",
        "                    if not time_diffs.empty:\n",
        "                        expected_interval = time_diffs.mode()[0]\n",
        "                        if pd.isna(expected_interval) or expected_interval == pd.Timedelta(seconds=0):\n",
        "                             # Fallback to a default interval if mode is not meaningful\n",
        "                             expected_interval = pd.Timedelta(minutes=10) # Default to 10 minutes, adjust as needed\n",
        "\n",
        "                        full_time_range = pd.date_range(start=start_time, end=end_time, freq=expected_interval)\n",
        "                        df = df.set_index('time').reindex(full_time_range).reset_index()\n",
        "                        df = df.rename(columns={'index': 'time'})\n",
        "                    else:\n",
        "                         print(f\"Warning: Could not infer time frequency for {os.path.basename(file_path)}. Skipping reindexing.\")\n",
        "\n",
        "            else:\n",
        "                print(f\"Warning: 'time_diff' column not found in {os.path.basename(file_path)}\")\n",
        "\n",
        "        else:\n",
        "             print(f\"Warning: 'time' column not found in {os.path.basename(file_path)}\")\n",
        "\n",
        "\n",
        "        # 4. Apply linear interpolation and discard files with excessive missing data\n",
        "        interpolated_df = df.copy()\n",
        "        for col in mask_cols:\n",
        "            if col in interpolated_df.columns:\n",
        "                # Calculate the size of continuous NaN blocks\n",
        "                # This requires identifying groups of consecutive NaNs\n",
        "                nan_groups = interpolated_df[col].isnull().astype(int).groupby(interpolated_df[col].notnull().astype(int).cumsum()).sum()\n",
        "\n",
        "                # Check if any continuous NaN block exceeds 24 hours of expected data points\n",
        "                # Assuming 10-minute interval, 24 hours is 24 * 60 / 10 = 144 data points\n",
        "                # This threshold needs to be adjusted based on your data's frequency\n",
        "                # Let's use the expected_interval inferred earlier\n",
        "                if 'expected_interval' in locals() and not pd.isna(expected_interval) and expected_interval != pd.Timedelta(seconds=0):\n",
        "                     max_allowed_nan_duration = pd.Timedelta(hours=24)\n",
        "                     max_allowed_nan_count = int(max_allowed_nan_duration / expected_interval)\n",
        "\n",
        "                     if any(nan_groups > max_allowed_nan_count):\n",
        "                         print(f\"Discarding file {os.path.basename(file_path)} due to excessive continuous missing data in column '{col}'.\")\n",
        "                         return None # Discard the file\n",
        "\n",
        "                interpolated_df[col] = interpolated_df[col].interpolate(method='linear')\n",
        "\n",
        "        # print(f\"Missing values after processing in {os.path.basename(file_path)}:\\n{interpolated_df.isnull().sum()}\")\n",
        "\n",
        "        return interpolated_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {os.path.basename(file_path)}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Main processing loop ---\n",
        "data_folder = '/content/drive/MyDrive/weather_5k' # Replace with your actual folder path\n",
        "output_folder = '/content/drive/MyDrive/processed_station_data' # Replace with your desired output folder path\n",
        "\n",
        "# Create output folder if it doesn't exist\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "    print(f\"Created output folder: {output_folder}\")\n",
        "\n",
        "processed_files_count = 0\n",
        "discarded_files_count = 0\n",
        "\n",
        "if not os.path.exists(data_folder):\n",
        "    print(f\"Error: Data folder not found at {data_folder}\")\n",
        "else:\n",
        "    file_list = [f for f in os.listdir(data_folder) if f.endswith('.csv')] # Assuming files are CSVs\n",
        "    total_files = len(file_list)\n",
        "    print(f\"Found {total_files} files to process.\")\n",
        "\n",
        "    for i, file_name in enumerate(file_list):\n",
        "        file_path = os.path.join(data_folder, file_name)\n",
        "        print(f\"Processing file {i+1}/{total_files}: {file_name}\")\n",
        "        processed_df = process_station_data(file_path)\n",
        "\n",
        "        if processed_df is not None:\n",
        "            # Save the processed DataFrame to a new file\n",
        "            output_file_path = os.path.join(output_folder, file_name)\n",
        "            processed_df.to_csv(output_file_path, index=False)\n",
        "            print(f\"Successfully processed and saved file: {file_name}\")\n",
        "            processed_files_count += 1\n",
        "        else:\n",
        "            discarded_files_count += 1\n",
        "\n",
        "    print(f\"\\nFinished processing. Successfully processed and saved {processed_files_count} files. Discarded {discarded_files_count} files.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}